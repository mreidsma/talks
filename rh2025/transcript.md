### Title Slide:

----

### Slide 1:

Before we start, I put together a quick guide with resources that we'll use during this workshop, as well as resources you can use to learn more. The URL is here, gvsu.edu/library/bias.
-----

### Slide 2:

Anyone besides me born before 1998, the year that Google launched? Search engines have existed for your entire lives. We use search for everything: finding restaurants and coffee shops, directions, book and movie recommendations (and reviews), and to settle an argument about what other TV that actor starred in last year. And research. We use it for research.

-----

### Slide 3:

Search engines spend a lot of time convincing us to trust them. Google has been the best at this - they even show you lots of answers eight on the search screen, so that it seems like Google is giving you the answer, when they might be pulling the content from Wikipedia, or an official website, or another service like Yelp.

Library search tools are even more interested in building trust. Anyone here ever make the pitch to another student that the library search is better because it only searches content that has been vetted by publishers and journal editors? Back in the early 2000s, Libraries often used the slogan "Better than Google" to describe the library and its search. They were liars. Library Search was terrible, and is no where near as useful as Google even now.
 
-----

### Slide 4:

Our culture has a pretty deeply held belief in the truth of things that come from computers. That's one reason why it's so difficult to explain to some people why you can't believe every post you see on Facebook, and why we have to teach folks to evaluate sources.

Today's conversation will be looking at some bad things that come through these algorithms, that are extra problematic because we lean toward trust. SO to start us off, let's laugh a little. This trust in computers is highlighted by a terrific British sketch comedy show Little Britain. I give you "Computer Says No."

-----

### Slide 5:

Search tools, and almost everything online at this point, are made up of algorithms. You've probably heard that term a lot. Anyone want to define it for us?

At its core, an algorithm is a set of instructions to turn inputs into outputs. That's pretty broad, and doesn't have to be computer code! Library cataloging directions are an algorithm. But when we move algorithms to computers, we can combine them in all kinds of ways and make them very complex. But the problem is that computers only understand one language: binary. It's all numbers. Computers were designed to do math, and that is still technically what they do best.

-----

### Slide 6:

This works for some things pretty well. Like, "What is the closest coffee shop to me" is something you can provide a factual answer for, because it can be measured. "What is the best coffee shop in Grand Rapids" is harder to quantify. What are the criteria? The most awards from Revue magazine? The most customers? The cheapest coffee? The most seating options? The "tastiest" coffee? How do you begin to quantify those judgments? 

This gets to be a problem with algorithms. OK Cupid is a dating site built on algorithms that rank attractiveness and compatibility, some attributes that are pretty hard to quantify. Christian Rudder, one of the founders, is pretty up front about how hard it is to translate these non-numeric ideas into numbers.

-----

### Slide 7:

Back in 1995, 3 years before Google was founded, the anthropologist Sherry Turkle noted this same issue: if you want to succeed on the web, you need to find ways to quantify the uncountable.

So software developers and designers and CEOs and entrepreneurs of all kinds started writing computer code that would translate things like attractiveness or relevance into quantifiable code. Seems like a lot of interesting decisions and judgments that would need to be made to make these translations into binary. Anyone here a computer science major? Do they have any ethics classes as a requirement?

-----

### Slide 8:

The thing is, we bring our experiences, our worldviews, our prejudices with us when we create things. We build those biases into our tools. When 22 year old upper middle class white software developers make decisions about how the world is, what they build is going to work really well for upper middle class white males, but maybe not for other people.

Like the gym in the UK that made an online form, and if you selected the salutation "Dr." - it automatically set your gender to "Male" and your card wouldn't let you access the women's locker room.


-----

### Slide 9:

Or the way Google Images originally would categorize images - here's a search from 2016. "Unprofessional hairstyles for work" showed a number of black women, while "professional hairstyles for work" showed all white women.

-----

### Slide 10:

That was 8 years ago, right? There probably aren't still issues, right?

Let's do an experiment. Go to Google Images and search for "Appalachian photography".

What do you see?

-----

### Slide 11:

The Blue Ridge mountains are a mountain range in the Appalachians. So the photos should bear some resemblance, right? Maybe have some overlap?

Now search for "Blue Ridge Photography"

-----

### Slide 12:

Google would call these "glitches," because the algorithm is not working the way they expected it to. But remember that we encode our values and biases into the things we make. My colleague Scarlet said that "glitches are the unintentional exposure of those values."

-----

### Slide 13:

So here's a search result from our old Library Search tool, Summon. It's a known item search for a book, and the only two results are the specific book and a book on mental illness. The juxtaposition of these two books , as the only results, implies a sort of equivalency between mental illness and LGBTQ Information Needs.

-----

### Slide 14:

This is how bias often presents itself. Not necessarily by explicitly coming out with biased results, but by making associations between things that highlight social biases.

-----

### Slide 15:

Here's another Library search tool called Primo, and it has an autocorrect algorithm that equated "new york waste" with "new york women."

-----

### Slide 16:

If you want to examine an algorithm thoroughly, nothing beats collecting a lot of sample results, but there are other places to look for problems. I wish I knew who the mastermind behind this site was, but Damn You, Auto Suggest is a Tumblr with the subtitle of "Primo Knows Best. Auto-suggest failures from library catalogs and databases." You can even submit your own.

Here Primo suggests that a search for "New York Waste" should have been a search for "New York Women."

-----

### Slide 17:

Here Primo thinks a search for Children's Literature could only be made better by transforming it into a search for Children's Sex Literature.

-----

### Slide 18:

Our old Library Search had a feature called Topic Explorer, that highlighted what a search was "about" - here it says that stress in the workplace is the same as "women in the workforce"

-----

### Slide 19:

Here a search for mental illness is equated to a "myth."

-----

### Slide 20:

The Birth of Feminism is equated to the birth of tragedy.

-----

### Slide 21:

Our current library search also has some of these features that pick ONE "answer" for your search. We don't have them turned on because of results like this one, where a search for "racism" suggests you start looking into scientific racism, a debunked white supremacist theory from the 19th century.
-----

### Slide 22:

A lot of these biased results come about because of a sort of "Mad Libs" effect, where designers and developers are trying to match patterns of words. But sometimes we can see the logic behind why a result shows up that doesn't involve words, like this one that shows Clarence Thomas, an African American Supreme Court justice, next to a research starter on "Sexual harassment." Students of history will know why that image was picked, because his confirmation hearing was the first mainstream televised accusation of sexual harassment that really stuck in the public consciousness. But these hearings happened at last a decade before most of today's college students were born, so they might just see the heading "Sexual harassment" with an illustration of a black man.

-----

### Slide 23:

All of the results I've shown you in the screenshots have been blocked by the vendors. But there are still issues, because the haven't looked into the biases that were embedded in their systems to begin with. Let's explore a bit, shall we?

Go to Davenport's website and search for the Birth of Tragedy. Let's identify how many different algorithms are on this results page. What ones are most at risk for exhibiting bias and why?

-----

### Slide 24:

Now search for "women in film." What do the algorithms say?

-----

### Slide 25:

Let's go now to our Library Search. Search for "women." What do the algorithms suggest?

-----

### Slide 26:

Besides the disastrous Research Starters algorithm, EBSCO recently launched an even worse feature called the Concept Map. We do not have this on, and you're about to see why. Let's go to Georgia State's website and open the concept map. No search for "Adolph Hitler" and examine all the results. Does this seem to cover all the concepts that explain Hitler?

-----

### Slide 27:

Certainly Google, with it's army of employees, doesn't have as many issues, right? Let's try Google search and see what all the buzz is about in it's AI search.

Start typing "Did Hitler" and see what the autosuggest tells us.

Select "did hitler do any good" and evaluate the AI result up top.

-----

### Slide 28:

# Thoughts?

-----

### Slide 29:

Thank you for listening.
